# -*- coding: utf-8 -*-
"""HW#1_DL_Vedasravas

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ib8sjnXQ2RZQBZLPAbtuZetzc1v1_7Lu
"""

# Library Import
import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
import seaborn as sns
import keras
import time
import pickle
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import SGD
from tensorflow.keras.layers import Dropout
from tensorflow.keras import Input
from sklearn.neural_network import MLPRegressor
from datetime import datetime

# Dataset Loading
data = pd.read_csv("/content/cancer_reg.csv", encoding='latin-1')
data.head()

data.shape

# Minimum and Maximum values in the dataset
min_values = data.min()
max_values = data.max()

print("Minimum values in the dataset:\n", min_values)
print("\nMaximum values in the dataset:\n", max_values)

print(data.isnull().sum())

# Dropping features
data_cleaned = data.drop(columns=['Geography', 'binnedInc', 'avgDeathsPerYear', 'BirthRate',
                           'studyPerCap', 'MedianAgeMale', 'MedianAge', 'PctNoHS18_24','PctSomeCol18_24','popEst2015'])

# Correlation matrix
corr_matrix = data.corr(numeric_only=True)

# Correlation with the target column
target_corr = corr_matrix['TARGET_deathRate'].sort_values(ascending=False)
print(target_corr)

# Filling blank spaces
data_cleaned.fillna(data_cleaned.mean(), inplace=True)

# Splitting features (X) and target (y) from cleaned data
X = data_cleaned.drop(columns=['TARGET_deathRate'])
y = data_cleaned['TARGET_deathRate']

# Check the remaining columns
print(data_cleaned.columns)

# Splitting
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)

# Standard scaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
print(X_train_scaled)
print(X_test_scaled)

# Using StandardScaler to normalize the target (y)
scaler_y = StandardScaler()
y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1)).flatten()
y_test_scaled = scaler_y.transform(y_test.values.reshape(-1, 1)).flatten()
print(y_train_scaled)
print(y_test_scaled)

# Linear Regression
model = LinearRegression()
model.fit(X_train, y_train)
# prediction on test data
y_test_pred = model.predict(X_test)
# Prediction on training data
y_train_pred = model.predict(X_train)

# Calculate R-squared and MSE for both training and testing sets
r2_train = r2_score(y_train, y_train_pred)
mse_train = mean_squared_error(y_train, y_train_pred)

r2_test = r2_score(y_test, y_test_pred)
mse_test = mean_squared_error(y_test, y_test_pred)
mae = mean_absolute_error(y_test, y_test_pred)

print(f"Training R-Squared: {r2_train:.4f}, Training MSE: {mse_train:.4f}")
print(f"Test R-Squared: {r2_test:.4f}, Test MSE: {mse_test:.4f}")

# Define function to build and train the linear regression model
def train_evaluate_linear_regression(X_train, y_train, X_test, y_test, learning_rate):
    model = Sequential()
    model.add(Dense(1, input_dim=X_train.shape[1]))  # Linear regression with one output

    # Compile the model with the specified learning rate
    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])

    # Train the model
    model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=0)

    # Predict on the test set
    y_test_pred = model.predict(X_test)

    # Calculate R-squared
    r2 = r2_score(y_test, y_test_pred)
    print(f"Learning Rate: {learning_rate} - Test R-Squared: {r2:.4f}")

    return r2

# Learning rates to try
learning_rates = [0.1, 0.01, 0.001, 0.0001]
results = {}

# Train and evaluate for each learning rate
for lr in learning_rates:
    print(f"\nTraining with Learning Rate: {lr}")
    r2 = train_evaluate_linear_regression(X_train, y_train, X_test, y_test, lr)
    results[lr] = r2

# Optional: Print all results
print("\nFinal R-Squared values for all learning rates:")
for lr, r2 in results.items():
    print(f"Learning Rate: {lr} - R-Squared: {r2:.4f}")

def test_model(model, X_train, y_train, X_test, y_test):
    # Fit the model on training data
    model.fit(X_train, y_train)

    # Make predictions on both training and test data
    y_train_pred = model.predict(X_train)
    y_test_pred = model.predict(X_test)

    # Calculate R-squared and MSE for both training and testing sets
    r2_train = r2_score(y_train, y_train_pred)
    mse_train = mean_squared_error(y_train, y_train_pred)

    r2_test = r2_score(y_test, y_test_pred)
    mse_test = mean_squared_error(y_test, y_test_pred)

    # Optionally calculate Mean Absolute Error (MAE)
    mae_test = mean_absolute_error(y_test, y_test_pred)

    # Print or return the results
    print(f"Training R-Squared: {r2_train:.4f}, Training MSE: {mse_train:.4f}")
    print(f"Test R-Squared: {r2_test:.4f}, Test MSE: {mse_test:.4f}")

    return {
        "r2_train": r2_train,
        "mse_train": mse_train,
        "r2_test": r2_test,
        "mse_test": mse_test,
        "mae_test": mae_test
    }

# Neural Network
def train_evaluate_dnn(hidden_layer_sizes, X_train_scaled, y_train, X_test_scaled, y_test):
    mlp = MLPRegressor(hidden_layer_sizes=hidden_layer_sizes, max_iter=2000, random_state=42)
    mlp.fit(X_train_scaled, y_train)

    y_test_pred = mlp.predict(X_test_scaled)
    r2 = r2_score(y_test, y_test_pred)
    mse = mean_squared_error(y_test, y_test_pred)
    print(f"DNN {hidden_layer_sizes} - Test R-Squared: {r2:.4f}, Test MSE: {mse:.4f}")
    return mlp
dnn_architectures = [(16,), (30, 8), (30, 16, 8), (30, 16, 8, 4)]
dnn_models = {}
for arch in dnn_architectures:
    print(f"Training DNN model with architecture {arch}...")
    model = train_evaluate_dnn(arch, X_train_scaled, y_train, X_test_scaled, y_test)
    dnn_models[arch] = model

def test_model(model, X_test_scaled, y_test):
    y_test_pred = model.predict(X_test_scaled)
    r2 = r2_score(y_test, y_test_pred)
    mse = mean_squared_error(y_test, y_test_pred)
    print(f"Model Test R-Squared: {r2:.4f}, Test MSE: {mse:.4f}")
    return r2, mse

# SGD optimizer
def train_evaluate_dnn_sgd_epochs(hidden_layer_sizes, X_train_scaled, y_train_scaled, X_test_scaled, y_test_scaled, learning_rate=0.001, max_epochs=100):
    mlp = MLPRegressor(hidden_layer_sizes=hidden_layer_sizes, solver='sgd', learning_rate_init=learning_rate, max_iter=1, random_state=42, warm_start=True)

    training_loss = []

    # Start time
    start_time = datetime.now()
    execution_start = time.time()
    print(f"Training started at: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")

    for epoch in range(1, max_epochs + 1):
        mlp.partial_fit(X_train_scaled, y_train_scaled)

        # Training
        y_train_pred_scaled = mlp.predict(X_train_scaled)

        # Inversing
        y_train_pred = scaler_y.inverse_transform(y_train_pred_scaled.reshape(-1, 1)).flatten()

        # MSE
        train_loss = mean_squared_error(y_train, y_train_pred)
        training_loss.append(train_loss)

        print(f"Epoch {epoch}/{max_epochs} - Training Loss: {train_loss:.4f}")

    # End time
    end_time = datetime.now()
    execution_end = time.time()
    total_time = execution_end - execution_start

    print(f"Training ended at: {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"Total training time: {total_time:.2f} seconds")

    # R-squared
    y_test_pred_scaled = mlp.predict(X_test_scaled)
    y_test_pred = scaler_y.inverse_transform(y_test_pred_scaled.reshape(-1, 1)).flatten()
    r2_test = r2_score(y_test, y_test_pred)

    print(f"DNN {hidden_layer_sizes} with Learning Rate {learning_rate} - Final Test R-squared: {r2_test:.4f}")

    return training_loss, r2_test

# Plotting
def plot_loss_for_learning_rate(learning_rate, architectures, X_train_scaled, y_train_scaled, X_test_scaled, y_test_scaled, max_epochs=100):
    plt.figure(figsize=(10, 6))

    r2_scores = {}

    for arch in architectures:
        print(f"\nTraining DNN model with architecture {arch} and learning rate {learning_rate}...")
        training_loss, r2_test = train_evaluate_dnn_sgd_epochs(arch, X_train_scaled, y_train_scaled, X_test_scaled, y_test_scaled, learning_rate, max_epochs)

        plt.plot(range(1, max_epochs + 1), training_loss, label=f'Arch {arch} (R2: {r2_test:.4f})')
        r2_scores[arch] = r2_test

    plt.xlabel('Epochs')
    plt.ylabel('Mean Squared Error (Training Loss)')
    plt.title(f'Training Loss and R-Squared for Learning Rate {learning_rate}')
    plt.legend()
    plt.grid(True)
    plt.show()

    # Printing R-squared for all architectures for the current learning rate
    print(f"\nR-squared values for Learning Rate {learning_rate}:")
    for arch, r2 in r2_scores.items():
        print(f"Architecture {arch}: R-squared = {r2:.4f}")

# Storing results to use them in plotting phase
results = {}

# DNN Architectures
dnn_architectures = [(16,), (30, 8), (30, 16, 8), (30, 16, 8, 4)]

# Learning rates to test
learning_rates = [0.1, 0.01, 0.001, 0.0001]

for lr in learning_rates:
    plot_loss_for_learning_rate(lr, dnn_architectures, X_train_scaled, y_train_scaled, X_test_scaled, y_test_scaled, max_epochs=100)